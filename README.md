# Adversarial Attack on MNIST with VAE and Classifier

This project demonstrates an adversarial attack on the MNIST dataset using a Variational Autoencoder (VAE) and a classifier. The attack method used is the Fast Gradient Sign Method (FGSM).

## Table of Contents

- [Introduction](#introduction)
- [Project Structure](#project-structure)
- [Installation](#installation)
- [Usage](#usage)
- [Results](#results)
- [Contributing](#contributing)
- [License](#license)

## Introduction

Adversarial attacks are techniques that involve manipulating input data to cause machine learning models to make incorrect predictions. In this project, we demonstrate how to implement FGSM on the MNIST dataset using a VAE and a simple neural network classifier.

## Project Structure
